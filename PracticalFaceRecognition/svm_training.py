import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
import cv2

class SVMTrainer:
    def __init__(self, patch_size=(64, 64)):
        """
        Kh·ªüi t·∫°o SVM Trainer
        
        Args:
            patch_size: K√≠ch th∆∞·ªõc patch (width, height)
        """
        self.patch_size = patch_size
        self.svm = None
        self.best_C = None
        self.best_accuracy = 0
        self.W = None
        self.b = None
        self.scaler = None  # S·∫Ω ƒë∆∞·ª£c set t·ª´ DataPreparation
        
    def train_svm(self, X_train, y_train, C=1.0, kernel='linear'):
        """
        Hu·∫•n luy·ªán SVM
        
        Args:
            X_train: D·ªØ li·ªáu hu·∫•n luy·ªán
            y_train: Nh√£n hu·∫•n luy·ªán
            C: Tham s·ªë regularization
            kernel: Lo·∫°i kernel ('linear', 'rbf', etc.)
            
        Returns:
            Trained SVM model
        """
        print(f"üöÄ Hu·∫•n luy·ªán SVM v·ªõi C={C}, kernel={kernel}...")
        
        # Kh·ªüi t·∫°o SVM
        self.svm = SVC(C=C, kernel=kernel, random_state=42)
        
        # Hu·∫•n luy·ªán
        self.svm.fit(X_train, y_train)
        
        print(f"‚úÖ SVM ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán!")
        print(f"   - S·ªë support vectors: {len(self.svm.support_vectors_)}")
        print(f"   - Support vector indices: {len(self.svm.support_)}")
        
        return self.svm
    
    def evaluate_svm(self, X_val, y_val):
        """
        ƒê√°nh gi√° SVM tr√™n t·∫≠p validation
        
        Args:
            X_val: D·ªØ li·ªáu validation
            y_val: Nh√£n validation
            
        Returns:
            accuracy: ƒê·ªô ch√≠nh x√°c
        """
        if self.svm is None:
            raise ValueError("SVM ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán!")
        
        # D·ª± ƒëo√°n
        y_pred = self.svm.predict(X_val)
        
        # T√≠nh accuracy
        accuracy = accuracy_score(y_val, y_pred)
        
        print(f"üìä K·∫øt qu·∫£ ƒë√°nh gi√°:")
        print(f"   - Accuracy: {accuracy:.4f}")
        print(f"   - Classification Report:")
        print(classification_report(y_val, y_pred))
        
        return accuracy
    
    def compute_hyperplane(self):
        """
        T√≠nh to√°n hyperplane W t·ª´ support vectors v√† alpha coefficients
        
        Returns:
            W: Weight vector c·ªßa hyperplane
            b: Bias term
        """
        if self.svm is None:
            raise ValueError("SVM ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán!")
        
        if self.svm.kernel != 'linear':
            print("‚ö†Ô∏è Ch·ªâ c√≥ th·ªÉ t√≠nh W cho linear kernel!")
            return None, None
        
        # C√°ch ƒë∆°n gi·∫£n h∆°n: s·ª≠ d·ª•ng coef_ v√† intercept_ c√≥ s·∫µn
        W = self.svm.coef_[0]  # Weight vector
        b = self.svm.intercept_[0]  # Bias term
        
        self.W = W
        self.b = b
        
        print(f"Hyperplane ƒë√£ ƒë∆∞·ª£c t√≠nh to√°n:")
        print(f"   - W shape: {W.shape}")
        print(f"   - Bias b: {b:.6f}")
        print(f"   - ||W||: {np.linalg.norm(W):.6f}")
        
        return W, b
    
    def predict_confidence(self, X):
        """
        D·ª± ƒëo√°n confidence score t·ª´ W v√† b
        
        Args:
            X: D·ªØ li·ªáu ƒë·∫ßu v√†o
            
        Returns:
            confidence_scores: ƒêi·ªÉm confidence
        """
        if self.W is None or self.b is None:
            raise ValueError("Hyperplane ch∆∞a ƒë∆∞·ª£c t√≠nh to√°n!")
        
        # T√≠nh confidence: f(x) = W^T * x + b
        confidence_scores = np.dot(X, self.W) + self.b
        
        return confidence_scores
    
    def visualize_weight_vector(self, title="Weight Vector W"):
        """
        Tr·ª±c quan h√≥a W nh∆∞ ·∫£nh - s·∫Ω th·∫•y h√¨nh ·∫£nh gi·ªëng m·∫∑t ng∆∞·ªùi
        
        Args:
            title: Ti√™u ƒë·ªÅ c·ªßa plot
        """
        if self.W is None:
            raise ValueError("Hyperplane ch∆∞a ƒë∆∞·ª£c t√≠nh to√°n!")
        
        # Reshape W v·ªÅ k√≠ch th∆∞·ªõc ·∫£nh
        W_img = self.W.reshape(self.patch_size)
        
        # Chu·∫©n h√≥a ƒë·ªÉ hi·ªÉn th·ªã
        W_normalized = (W_img - W_img.min()) / (W_img.max() - W_img.min())
        
        plt.figure(figsize=(10, 8))
        
        # Plot W
        plt.subplot(2, 2, 1)
        plt.imshow(W_normalized, cmap='gray')
        plt.title(f'{title} (Normalized)')
        plt.colorbar()
        plt.axis('off')
        
        # Plot W v·ªõi colormap kh√°c
        plt.subplot(2, 2, 2)
        plt.imshow(W_img, cmap='RdBu_r', vmin=-np.abs(W_img).max(), vmax=np.abs(W_img).max())
        plt.title(f'{title} (Centered)')
        plt.colorbar()
        plt.axis('off')
        
        # Plot histogram c·ªßa W
        plt.subplot(2, 2, 3)
        plt.hist(self.W, bins=50, alpha=0.7, color='blue')
        plt.title('Histogram of W values')
        plt.xlabel('Weight value')
        plt.ylabel('Frequency')
        
        # Plot magnitude c·ªßa W theo v·ªã tr√≠
        plt.subplot(2, 2, 4)
        W_magnitude = np.abs(W_img)
        plt.imshow(W_magnitude, cmap='hot')
        plt.title('Magnitude of W')
        plt.colorbar()
        plt.axis('off')
        
        plt.tight_layout()
        plt.show()
        
        print(f"Weight vector ƒë√£ ƒë∆∞·ª£c tr·ª±c quan h√≥a!")
        print(f"   - Min W: {self.W.min():.6f}")
        print(f"   - Max W: {self.W.max():.6f}")
        print(f"   - Mean |W|: {np.mean(np.abs(self.W)):.6f}")
    
    def find_best_C(self, X_train, y_train, X_val, y_val, C_values=None):
        """
        T√¨m gi√° tr·ªã C t·ªët nh·∫•t b·∫±ng c√°ch th·ª≠ c√°c gi√° tr·ªã kh√°c nhau
        
        Args:
            X_train, y_train: D·ªØ li·ªáu hu·∫•n luy·ªán
            X_val, y_val: D·ªØ li·ªáu validation
            C_values: Danh s√°ch gi√° tr·ªã C ƒë·ªÉ th·ª≠
            
        Returns:
            best_C: Gi√° tr·ªã C t·ªët nh·∫•t
            best_accuracy: Accuracy t·ªët nh·∫•t
        """
        if C_values is None:
            C_values = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0]
        
        print(f"T√¨m gi√° tr·ªã C t·ªët nh·∫•t...")
        print(f"   - C√°c gi√° tr·ªã C s·∫Ω th·ª≠: {C_values}")
        
        accuracies = []
        best_C = None
        best_accuracy = 0
        
        for C in C_values:
            print(f"\nTh·ª≠ C = {C}...")
            
            # Hu·∫•n luy·ªán SVM v·ªõi C hi·ªán t·∫°i
            self.train_svm(X_train, y_train, C=C)
            
            # ƒê√°nh gi√°
            accuracy = self.evaluate_svm(X_val, y_val)
            accuracies.append(accuracy)
            
            # C·∫≠p nh·∫≠t best
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_C = C
                print(f"   C = {C} l√† t·ªët nh·∫•t cho ƒë·∫øn nay!")
        
        # Hu·∫•n luy·ªán l·∫°i v·ªõi C t·ªët nh·∫•t
        print(f"\nHu·∫•n luy·ªán l·∫°i v·ªõi C t·ªët nh·∫•t = {best_C}")
        self.train_svm(X_train, y_train, C=best_C)
        self.compute_hyperplane()
        
        # V·∫Ω bi·ªÉu ƒë·ªì accuracy vs C
        plt.figure(figsize=(10, 6))
        plt.plot(C_values, accuracies, 'bo-', linewidth=2, markersize=8)
        plt.axvline(x=best_C, color='red', linestyle='--', label=f'Best C = {best_C}')
        plt.xlabel('C (Regularization parameter)')
        plt.ylabel('Validation Accuracy')
        plt.title('Accuracy vs Regularization Parameter C')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.xscale('log')
        plt.show()
        
        self.best_C = best_C
        self.best_accuracy = best_accuracy
        
        print(f"\nK·∫øt qu·∫£ t√¨m C t·ªët nh·∫•t:")
        print(f"   - Best C: {best_C}")
        print(f"   - Best Accuracy: {best_accuracy:.4f}")
        
        return best_C, best_accuracy
    
    def analyze_regularization_effect(self, X_train, y_train, X_val, y_val):
        """
        Ph√¢n t√≠ch hi·ªáu ·ª©ng c·ªßa regularization
        
        Args:
            X_train, y_train: D·ªØ li·ªáu hu·∫•n luy·ªán
            X_val, y_val: D·ªØ li·ªáu validation
        """
        print("Ph√¢n t√≠ch hi·ªáu ·ª©ng regularization...")
        
        C_values = [0.01, 0.1, 1.0, 10.0, 100.0]
        W_norms = []
        accuracies = []
        
        for C in C_values:
            print(f"\nPh√¢n t√≠ch C = {C}...")
            
            # Hu·∫•n luy·ªán SVM
            self.train_svm(X_train, y_train, C=C)
            
            # T√≠nh W
            W, b = self.compute_hyperplane()
            
            # T√≠nh ||W||
            W_norm = np.linalg.norm(W)
            W_norms.append(W_norm)
            
            # T√≠nh accuracy
            accuracy = self.evaluate_svm(X_val, y_val)
            accuracies.append(accuracy)
            
            # Tr·ª±c quan h√≥a W
            self.visualize_weight_vector(f"Weight Vector W (C={C})")
        
        # V·∫Ω bi·ªÉu ƒë·ªì ph√¢n t√≠ch
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # Accuracy vs C
        axes[0].plot(C_values, accuracies, 'bo-', linewidth=2, markersize=8)
        axes[0].set_xlabel('C (Regularization parameter)')
        axes[0].set_ylabel('Validation Accuracy')
        axes[0].set_title('Accuracy vs Regularization Parameter C')
        axes[0].grid(True, alpha=0.3)
        axes[0].set_xscale('log')
        
        # ||W|| vs C
        axes[1].plot(C_values, W_norms, 'ro-', linewidth=2, markersize=8)
        axes[1].set_xlabel('C (Regularization parameter)')
        axes[1].set_ylabel('||W|| (Weight vector norm)')
        axes[1].set_title('Weight Vector Norm vs Regularization Parameter C')
        axes[1].grid(True, alpha=0.3)
        axes[1].set_xscale('log')
        
        plt.tight_layout()
        plt.show()
        
        print(f"\nK·∫øt lu·∫≠n ph√¢n t√≠ch regularization:")
        print(f"   - C nh·ªè ‚Üí ||W|| nh·ªè ‚Üí Regularization m·∫°nh ‚Üí W gi·ªëng m·∫∑t h∆°n")
        print(f"   - C l·ªõn ‚Üí ||W|| l·ªõn ‚Üí Regularization y·∫øu ‚Üí W c√≥ th·ªÉ overfit")

if __name__ == "__main__":
    # Demo s·ª≠ d·ª•ng
    from data_preparation import DataPreparation
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu
    data_prep = DataPreparation()
    X_train, X_val, y_train, y_val = data_prep.prepare_data()
    
    # Hu·∫•n luy·ªán SVM
    svm_trainer = SVMTrainer()
    
    # T√¨m C t·ªët nh·∫•t
    best_C, best_acc = svm_trainer.find_best_C(X_train, y_train, X_val, y_val)
    
    # Ph√¢n t√≠ch regularization
    svm_trainer.analyze_regularization_effect(X_train, y_train, X_val, y_val) 